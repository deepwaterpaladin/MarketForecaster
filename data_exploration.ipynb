{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Partition Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/train.parquet/\"\n",
    "train_files = sorted(glob(train_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([pd.read_parquet(file) for file in train_files], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train DataFrame shape:\", train_df.shape)\n",
    "print(\"Columns in Train DataFrame:\", train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_path = \"data/lags.parquet/date_id=0\"\n",
    "lags_df = pd.read_parquet(lags_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = \"data/features.csv\"\n",
    "responders_path = \"data/responders.csv\" \n",
    "\n",
    "features_metadata = pd.read_csv(features_path)\n",
    "responders_metadata = pd.read_csv(responders_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lags DataFrame shape:\", lags_df.shape)\n",
    "print(\"Features Metadata shape:\", features_metadata.shape)\n",
    "print(\"Responders Metadata shape:\", responders_metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Data Preview:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nLagged Responders Preview:\")\n",
    "print(lags_df.head())\n",
    "\n",
    "print(\"\\nFeatures Metadata Preview:\")\n",
    "print(features_metadata.head())\n",
    "\n",
    "print(\"\\nResponders Metadata Preview:\")\n",
    "print(responders_metadata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of setting data types for optimization\n",
    "train_df = train_df.astype({\n",
    "    'date_id': 'int32',\n",
    "    'time_id': 'int32',\n",
    "    'symbol_id': 'int32',\n",
    "    'weight': 'float32',\n",
    "    **{f'feature_{i:02}': 'float32' for i in range(79)},\n",
    "    **{f'responder_{i}': 'float32' for i in range(9)}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Check Data Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary Statistics for Train Data:\")\n",
    "print(train_df.describe().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Evaluate Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipping first five feature cols due to NaN being the main/only value.\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(5, 10):\n",
    "    plt.subplot(2, 3, i - 4)\n",
    "    sns.histplot(train_df[f'feature_{i:02}'], kde=True, bins=50)\n",
    "    plt.title(f'Feature {i:02} Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for a subset of features and responders\n",
    "correlation_matrix = train_df[[f'feature_{i:02}' for i in range(5)] + [f'responder_{i}' for i in range(9)]].corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix for Selected Features and Responders\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot `responder_6` across a sample of time points for a subset of symbols\n",
    "sample_symbols = train_df['symbol_id'].sample(5, random_state=0).unique()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "for symbol in sample_symbols:\n",
    "    symbol_data = train_df[train_df['symbol_id'] == symbol]\n",
    "    plt.plot(symbol_data['date_id'], symbol_data['responder_6'], label=f'Symbol {symbol}')\n",
    "plt.title(\"Responder_6 Over Time for Sample Symbols\")\n",
    "plt.xlabel(\"Date ID\")\n",
    "plt.ylabel(\"Responder_6\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Investigate `Weight` Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_df['weight'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Weight\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipping_counts = train_df['responder_6'].value_counts().loc[[-5, 5]]\n",
    "print(\"Clipping Boundary Counts for Responder_6:\\n\", clipping_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Explore Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = train_df.isnull().sum()\n",
    "print(\"Missing Values by Column:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Weight Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(lags_df, on=['date_id', 'symbol_id'], suffixes=('', '_lag1'))\n",
    "\n",
    "print(\"Train Data with Lagged Responders:\")\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5, 10):\n",
    "    feature = f'feature_{i:02}'\n",
    "    train_df[f'{feature}_rolling_mean'] = train_df.groupby('symbol_id')[feature].transform(lambda x: x.rolling(window=5, min_periods=1).mean())\n",
    "    train_df[f'{feature}_rolling_std'] = train_df.groupby('symbol_id')[feature].transform(lambda x: x.rolling(window=5, min_periods=1).std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['feature_5_6_interaction'] = train_df['feature_05'] * train_df['feature_06']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Weight Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['normalized_weight'] = train_df.groupby('date_id')['weight'].transform(lambda x: x / x.sum())\n",
    "\n",
    "print(\"Check Weight Normalization:\")\n",
    "print(train_df.groupby('date_id')['normalized_weight'].sum().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns after Feature Engineering:\")\n",
    "print(train_df.columns)\n",
    "\n",
    "print(\"\\nSample of Feature-Engineered Data:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = train_df.corr()\n",
    "\n",
    "# Focus on correlations with responder_6\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix[['responder_6']], annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation with Responder 6')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for highly correlated features\n",
    "high_correlation_features = correlation_matrix['responder_6'].nlargest(6).index.tolist()\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(high_correlation_features[1:]):  # Skip responder_6 itself\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.scatterplot(x=train_df[feature], y=train_df['responder_6'], alpha=0.5)\n",
    "    plt.title(f'Responder 6 vs {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the training data\n",
    "train_df = pd.read_parquet('data/train.parquet')\n",
    "\n",
    "# Check the initial size of the DataFrame\n",
    "print(f'Initial number of rows: {len(train_df)}')\n",
    "\n",
    "# Separate features and target\n",
    "X = train_df[[f'feature_{i:02}' for i in range(79)]]\n",
    "y = train_df['responder_6']\n",
    "\n",
    "# Check for NaN values in X and y\n",
    "print(f'NaN values in X: {X.isna().sum().sum()}')\n",
    "print(f'NaN values in y: {y.isna().sum()}')\n",
    "\n",
    "# Impute missing values in features\n",
    "imputer = SimpleImputer(strategy='mean')  # You can change this strategy if needed\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Check for remaining NaN values\n",
    "print(f'NaN values in X after imputation: {pd.DataFrame(X_imputed).isna().sum().sum()}')\n",
    "\n",
    "# Split the data into training and validation sets (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "print(f'R-squared: {r2:.4f}')\n",
    "print(f'Mean Absolute Error: {mae:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_feature_combinations(n):\n",
    "    # Generate feature names\n",
    "    feature_names = [f'feature_{str(i).zfill(2)}' for i in range(5, n + 1)]\n",
    "    \n",
    "    # Store combinations in a dictionary\n",
    "    feature_sets = {}\n",
    "    \n",
    "    # Generate all combinations of features using a single loop\n",
    "    index = 1\n",
    "    for r in range(1, len(feature_names) + 1):\n",
    "        feature_sets.update({\n",
    "            f'Set_{index}': list(combo) for index, combo in enumerate(itertools.combinations(feature_names, r), start=index)\n",
    "        })\n",
    "        index += len(feature_names) // r  # Adjust index for the next set\n",
    "\n",
    "    c = {}\n",
    "    for k, v in feature_sets.items():\n",
    "        if len(v)>3:\n",
    "            c[k] = v\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = generate_feature_combinations(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Sample DataFrame df containing features and target variable\n",
    "features = ['feature_0', 'feature_1', 'feature_02', 'feature_03', 'feature_04']\n",
    "target = 'responder_6'\n",
    "\n",
    "# Define different combinations of features\n",
    "# feature_sets = {\n",
    "#     'Set_A': ['feature_05', 'feature_16', 'feature_09'],\n",
    "#     'Set_B': ['feature_07', 'feature_08'],\n",
    "#     'Set_C': ['feature_21', 'feature_22', 'feature_23', 'feature_24']\n",
    "# }\n",
    "\n",
    "results = {}\n",
    "for set_name, features in feature_sets.items():\n",
    "    # Prepare X and y\n",
    "    X = train_df[features]  # Ensure you drop NaNs based on selected features\n",
    "    y = train_df.loc[X.index, target]  # Align y with X\n",
    "    imputer = SimpleImputer(strategy='mean')  # You can change this strategy if needed\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "    # Split the data into training and validation sets (80/20 split)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize the Linear Regression model\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Store results\n",
    "    results[set_name] = {\n",
    "        'R-squared': r2_score(y_val, y_pred),\n",
    "        'Mean Absolute Error': mean_absolute_error(y_val, y_pred)\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "for set_name, metrics in results.items():\n",
    "    print(f\"{set_name}: R-squared = {metrics['R-squared']:.4f}, MAE = {metrics['Mean Absolute Error']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Select features (you can adjust based on correlation results)\n",
    "selected_features = ['feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09']  # Adjust as needed\n",
    "X = train_df[selected_features]\n",
    "y = train_df['responder_6']\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the OLS model\n",
    "# ols_model = sm.OLS(y, X).fit()\n",
    "ols_model = sm.OLS(y, X).fit(method='qr')\n",
    "\n",
    "# Output the summary\n",
    "print(ols_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with responder_6\n",
    "correlations = train_df.corr()['responder_6'].drop('responder_6')\n",
    "\n",
    "# Filter features with a high correlation\n",
    "high_corr_features = correlations[abs(correlations) > 0.3].index.tolist()\n",
    "print(\"High Correlation Features with Responder 6:\")\n",
    "print(high_corr_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Run linear regression for each high correlation feature\n",
    "for feature in high_corr_features:\n",
    "    X = train_df[[feature]]\n",
    "    y = train_df['responder_6']\n",
    "    \n",
    "    # Add a constant to the model (intercept)\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Fit the OLS model\n",
    "    ols_model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # Store the summary statistics\n",
    "    results.append({\n",
    "        'Feature': feature,\n",
    "        'Coefficient': ols_model.params[feature],\n",
    "        'P-value': ols_model.pvalues[feature],\n",
    "        'R-squared': ols_model.rsquared\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by R-squared value in descending order\n",
    "results_df = results_df.sort_values(by='R-squared', ascending=False)\n",
    "\n",
    "# Display the results\n",
    "print(\"Linear Regression Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the coefficients of the top features\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=results_df.head(10))\n",
    "plt.title('Top Features from Linear Regression against Responder 6')\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
